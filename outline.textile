h1. Backing up your app

h2. What are we going to back up?

* data - your database
* code - source control repo
* user contributed content - avatars, pics, posts, everything that's not in your DB
* your server configuration

h2. General backup principles

* automate backups
* test recovery
* automate recovery
* save everything.  Prune it later
* Just do it!

h2. Backup options

* Offsite
* Be paranoid about losing data, not about people stealing it
* Price
* Consider using more than one backup solution.

h2. Amazon S3

@sudo gem install aws-s3@

h2. Amazon S3

set up two environment variables,
@AMAZON_ACCESS_KEY_ID@ and @AMAZON_SECRET_ACCESS_KEY@

h2. Amazon S3

<pre>
#!/usr/bin/env ruby

require 'rubygems'
require 'aws/s3'
include AWS::S3

AWS::S3::Base.establish_connection!(
    :access_key_id     => ENV['AMAZON_ACCESS_KEY_ID'],
    :secret_access_key => ENV['AMAZON_SECRET_ACCESS_KEY']
)

bucket = ARGV[0]
file = ARGV[1]

Bucket.create(bucket)
S3Object.store(file, File.open(file), bucket)
</pre>

h2. backing up your DB

This is the most important part

Don't be these guys: 

!file:///Users/spatten/versioned/spattendesign/presentations/backing_up_your_app/journalspace.png!

h3. Solution 1

# Use EC2OnRails
# Go have a beer

http://github.com/pauldowman/ec2onrails

http://pauldowman.com

h3. Solution 2

Roll your own (i.e. Borrow Paul's backup code).

Here's how you do it in MySql.

h3. Full Backup

Gives you a snapshot of the database

@mysqldump@

h3. Full invocation

<pre>
mysqldump --quick --single-transaction --create-options -u<username> --flush-logs --master-data=2 --delete-master-logs \
  -p'<mysql password>' <database name> | gzip > <dump file>
</pre>
    
h3. Hmmm.  Think we should automate that command?

h3. Script

<pre>
#!/usr/bin/env ruby
require "common"
begin
  FileUtils.mkdir_p @temp_dir
  # assumes the bucket's empty
  dump_file = "#{@temp_dir}/dump.sql.gz"
  cmd = "mysqldump --quick --single-transaction --create-options " +
  "-u#{@mysql_user} --flush-logs --master-data=2 " +
  "--delete-master-logs"
  cmd += " -p'#{@mysql_password}'" unless @mysql_password.nil?
  cmd += " #{@mysql_database} | gzip > #{dump_file}"
  run(cmd)
  AWS::S3::S3Object.store(File.basename(dump_file), open(dump_file),@s3_bucket)
ensure
  FileUtils.rm_rf(@temp_dir)
end
</pre>

h3. Incremental backup

* backing up all of the binary logs
* flushing the logs you just backed up

h3. Binary log setup

Put this line in @my.cnf@:

@log_bin = /var/db/mysql/binlog/mysql-bin@

Give the user RELOAD and SUPER privileges:
<pre>
GRANT RELOAD ON *.* TO 'user_name'@'%' IDENTIFIED BY 'password';
GRANT SUPER ON *.* TO 'user_name'@'%' IDENTIFIED BY 'password';
</pre>

h3. Script

<pre>
#!/usr/bin/env ruby
require "common"
begin
  FileUtils.mkdir_p @temp_dir
  execute_sql "flush logs"
  logs = Dir.glob("#{@mysql_bin_log_dir}/mysql-bin.[0-9]*").sort
  logs_to_archive = logs[0..-2] # all logs except the last
  logs_to_archive.each do |log|
  # The following executes once for each filename in logs_to_archive
  AWS::S3::S3Object.store(File.basename(log), open(log), @s3_bucket)
  end
  execute_sql "purge master logs to '#{File.basename(logs[-1])}'"
ensure
  FileUtils.rm_rf(@temp_dir)
end
</pre>


h3. Stick it in your cron

Example cron entry:
<pre>
# Incremental backup every 10 minutes
*/10 * * * * root /usr/local/bin/incremental_backup.rb
# Full backup every day at 05:01
1 5 * * * root /usr/local/bin/full_backup.rb
</pre>

h3. Pruning your backups

If you run a lot of backups, then you'll end up with a ridiculous number of files.

Prune them every once in a while.

I tend to keep hourly backups for the last month, and daily backups forever.

h2. Backing up source control

h3. Version Control Survey

* Git
* SVN
* CVS
* PerForce
* Mercurial

h3. Solution 1

# Use Github
# Go have a beer

h3. Solution 2

Back up SVN to S3

h3. full and incremental backups

Full backups give you a complete snapshot of your repository
They can get very big

Incremental give you a diff of a commit

I tend to run a full backup every 50th commit or so.

h3. Script

<pre>
  def create_full_dump
    puts "Creating full dump for revision #{@rev}"
    STDOUT.flush
    cmd = "/usr/local/bin/svnadmin dump '#{@repos}' --revision " + 
          "'0:#{@rev}' > #{filename_with_path}"
    `#{cmd}`
    if @zip_full
      `gzip #{filename_with_path}`
    end
  end
  
  def create_incremental_dump
    cmd = "/usr/local/bin/svnadmin dump '#{@repos}' --revision " + 
          "'#{@rev}' --incremental > '#{filename_with_path}'"
    `#{cmd}`
    if @zip_incremental
      `gzip #{filename_with_path}`
    end
  end
  
  def cleanup
    File.unlink(filename_with_path(true))
  end  
</pre>

Full script: http://github.com/spatten/backing-up-your-app

h3. Hook or cron job?

h4. Hook

instant backup
post-commit hooks can fail

h4. Cron job

You aren't backed up until the script runs

Either way, make sure the script looks for any missing commits and uploads them too.

h2. Backing up user contributed content

Users upload an image, files, something that you are not storing in your DB.

You need to back this up too.

These are typically just files in a directory.  

Grab all of the files in those directories and put them on S3.

h3. s3sync

http://s3sync.net

Ruby script to back up to an S3 bucket

@rsync@ like syntax

h3. @sync_multiple_directories.rb@ from The S3 Cookbook

Create a YAML file that looks something like this:
<pre>
avatars:
  :directory: /mnt/app/shared/images/avatars
  :bucket: socialbandwagon_avatars_backup
  
pictures:
  :directory: /mnt/app/shared/images/pictures
  :bucket: socialbandwagon_pictures_backup
</pre>

h3. Run it like this

@sync_multiple_directories.rb multi_sync.yml@

(the code is at "http://github.com/spatten/backing-up-your-app":http://github.com/spatten/backing-up-your-app)

h3. Stick it in your cron

<pre>
15 4 * * * /mnt/app/current/script/sync_multiple_directories.rb /mnt/app/current/config/multi_sync.yml _>> \
/mnt/app/current/log/s3backup.log 2>&1
</pre>

h2. Backing up your server configuration

How many people do this?

How long would it take you to set up a server from scratch?

For me, even with ec2onrails, it's about 1.5 hours, start to finish, if everything goes perfectly.

Probably about three if I was going "Oh god.  The site is down!" the whole time.

h2. Creating an AMI on EC2

http://docs.amazonwebservices.com/AWSEC2/2007-08-29/GettingStartedGuide/creating-an-image.html

# Create an image (AMI) - This creates a snapshot of your server.  You can specify what directories to ignore.
# upload it to S3 - pick a bucket and upload the snapshot to S3
# Register the AMI - Once you register it, you can create a new EC2 instance that is an exact copy of the snapshot

VMWare or Xen will do the same thing for you

h2.

Thanks
